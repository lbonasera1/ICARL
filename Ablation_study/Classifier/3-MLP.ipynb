{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbT_HjsKuYVh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader, random_split, ConcatDataset\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7k7VhUovuYVx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\"\"\"\n",
    "Credits to @hshustc\n",
    "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=100):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)   # icarl uses bias!\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        #self.fc = nn.Linear(64, num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "                                         nn.Linear(64,82),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(82,100),\n",
    "                                         )\n",
    "        self.flag = True\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def set_flag(self, flag):\n",
    "        self.flag = flag\n",
    "        return 0     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if self.flag is True:\n",
    "            x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def resnet32MLP(pretrained=False, **kwargs):\n",
    "    n = 5\n",
    "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5YKmDdeuYV4"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' \n",
    "\n",
    "NUM_CLASSES = 100  \n",
    "\n",
    "BATCH_SIZE = 128     \n",
    "                     \n",
    "K = 2000              \n",
    "LR = 2.0            \n",
    "LR_L2 = 0.25\n",
    "LR_CE = 0.1\n",
    "LR_LFC = 0.1\n",
    "LR_L1 = 0.025        \n",
    "MOMENTUM = 0.9       \n",
    "WEIGHT_DECAY = 1e-5  \n",
    "WEIGHT_DECAY_CE = 5e-4\n",
    "\n",
    "NUM_EPOCHS = 60      \n",
    "MILESTONES = [49, 63]       \n",
    "MILESTONES_L2 = [30, 45]\n",
    "MILESTONES_CE = [50]\n",
    "MILESTONES_LFC = [40]  \n",
    "GAMMA = 0.2          \n",
    "GAMMA_CE = 0.2\n",
    "GAMMA_LFC = 0.2\n",
    "\n",
    "LOG_FREQUENCY = 10\n",
    "SEED = 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYVKtvPZuYV-"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(0.5),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
    "                                                           std=[0.2673, 0.2564, 0.2761])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
    "                                                          std=[0.2673, 0.2564, 0.2761])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LmbJeBUnuYWE",
    "outputId": "fba0ffb3-2b08-438b-cd2e-6a59fb318b43"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clone github repository with data\n",
    "if not os.path.isdir('./ICARL'):\n",
    "  !git clone https://github.com/lbonasera1/ICARL.git\n",
    "\n",
    "train_dataset = CIFAR100(root='./ICARL/data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = CIFAR100(root='./ICARL/data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_subsets = []\n",
    "test_subsets = []\n",
    "batch_classes = []\n",
    "class_indexes = [None] * NUM_CLASSES  \n",
    "random.seed(SEED)\n",
    "L = list(range(100))\n",
    "random.shuffle(L)\n",
    "\n",
    "for i in range(10):\n",
    "  # random extract 10 classes from 100\n",
    "  batch_classes.append([L.pop() for _ in range(10)])\n",
    "\n",
    "  # search and collect train and exemplars indices for i-batch\n",
    "  train_indices = []\n",
    "  for target in batch_classes[i]:\n",
    "    tmp = []\n",
    "    for idx, val in enumerate(train_dataset.targets):\n",
    "      if val == target:\n",
    "        train_indices.append(idx)\n",
    "        tmp.append(idx)\n",
    "    random.shuffle(tmp)\n",
    "    class_indexes[target] = tmp\n",
    "\n",
    "  random.shuffle(train_indices)\n",
    "  # create subset from indices\n",
    "  subset = Subset(train_dataset, train_indices)\n",
    "  train_subsets.append(subset)\n",
    "\n",
    "  # search and collect train indices for i-batch\n",
    "  test_indices = []\n",
    "  for target in batch_classes[i]:\n",
    "    for idx, val in enumerate(test_dataset.targets):\n",
    "      if target == val:\n",
    "         test_indices.append(idx)\n",
    "\n",
    "  random.shuffle(test_indices)\n",
    "  # create subset from indices\n",
    "  subset = Subset(test_dataset, test_indices)\n",
    "  test_subsets.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYGdPyAMuYWL"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(filepath):\n",
    "  model = torch.load(filepath)\n",
    "  for parameter in model.parameters():\n",
    "      parameter.requires_grad = False\n",
    "  model.eval()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POO24gA-uYWR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def constrExemplars(exemplars, classes, class_indexes, model, m):\n",
    "  pdist = nn.PairwiseDistance(p=2)\n",
    "  model.train(False)\n",
    "  model.set_flag(False)\n",
    "  class_means = torch.empty((0, 64)).cuda()\n",
    "  with torch.no_grad():\n",
    "    # compute mean for each class\n",
    "    for c in classes:\n",
    "      indexes = copy.deepcopy(class_indexes[c])\n",
    "      features = torch.empty((0, 64)).cuda()\n",
    "      # image set of class c\n",
    "      subset = Subset(train_dataset, indexes)\n",
    "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
    "      for image, label in dataLoader:\n",
    "        image = image.to(DEVICE)\n",
    "        # extract features\n",
    "        output = model(image)\n",
    "        # L2 normalization of feature vector\n",
    "        output = nn.functional.normalize(output, p=2, dim=1)\n",
    "        features = torch.cat((features, output))\n",
    "      \n",
    "      class_mean = torch.mean(features, 0)\n",
    "      class_mean = nn.functional.normalize(class_mean, p=2, dim=0)\n",
    "      class_mean = class_mean.view(-1, 64)\n",
    "      class_means = torch.cat((class_means, class_mean))\n",
    "      current_features = torch.empty((0, 64)).cuda()\n",
    "      exemplars_indexes = []\n",
    "      for k in range(m):\n",
    "        current_sum = torch.sum(current_features, 0)\n",
    "        current_sum = torch.add(features, current_sum.repeat(features.size(0), 1))\n",
    "        current_mean = current_sum * (1.0/(k+1))\n",
    "        current_mean = nn.functional.normalize(current_mean, p=2, dim=1)\n",
    "        distances = pdist(current_mean, class_mean)\n",
    "        index = torch.argmin(distances).item()   \n",
    "        phi = features[index].view(-1, 64)\n",
    "        # collecting chosen features\n",
    "        current_features = torch.cat((current_features, phi))\n",
    "        # removing chosen features\n",
    "        features = torch.cat((features[:index], features[index+1:]))\n",
    "        exemplars_indexes.append(indexes.pop(index))  \n",
    "      exemplars[c] = exemplars_indexes\n",
    "  model.set_flag(True)\n",
    "  model.train()\n",
    "  return class_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-7-S9QAuYWY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def reduceExemplars(exemplars, classes, m):\n",
    "  for c in classes:\n",
    "    exemplars[c] = exemplars[c][:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azquhvHKuYWe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def distillationLossCE(outputs, outputs_old, labels_old):\n",
    "  loss = torch.empty([0, 1]).cuda()\n",
    "  weights = torch.nn.functional.softmax(outputs_old, dim=1)\n",
    "  logs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "  results = torch.mul(weights, -logs)\n",
    "  mean = torch.mean(results, dim=0)\n",
    "  for k in labels_old:\n",
    "    loss = torch.cat((loss, mean[k].view(-1, 1)))\n",
    "  loss = torch.sum(loss)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "340Kpz1vuYWk"
   },
   "outputs": [],
   "source": [
    "\n",
    "def icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, criterion_dist):\n",
    "  net = resnet32MLP()\n",
    "  net = net.to(DEVICE)\n",
    "  cudnn.benchmark\n",
    "  batches_accuracy = []\n",
    "  labels_old = []\n",
    "  test_subList = []\n",
    "  exemplars = [None] * NUM_CLASSES\n",
    "\n",
    "  # iterate over class batches\n",
    "  for i in range(10):\n",
    "    train_clf_loss = []\n",
    "    train_dist_loss = []\n",
    "    criterion_clf = nn.CrossEntropyLoss()\n",
    "    # concatenate test classes\n",
    "    test_subList.append(test_subsets[i])\n",
    "    test_subset = ConcatDataset(test_subList)\n",
    "    # adding exemplars to train subset\n",
    "    train_subset = train_subsets[i]\n",
    "    if i > 0:\n",
    "      # get old labels\n",
    "      for j in batch_classes[i-1]:\n",
    "        labels_old.append(j)\n",
    "      train_subList = []\n",
    "      for k in labels_old:\n",
    "        train_subList = train_subList + exemplars[k]\n",
    "      random.shuffle(train_subList)  \n",
    "      subset = Subset(train_dataset, train_subList)\n",
    "      train_subset = ConcatDataset([train_subset, subset])\n",
    "    # initializate dataloader and variables\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    parameters_to_optimize = net.parameters()\n",
    "    optimizer = optim.SGD(parameters_to_optimize, lr=LR_CE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY_CE)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES_CE, gamma=GAMMA_CE)\n",
    "      \n",
    "    current_step = 0\n",
    "    # Start iterating over the epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
    "      # Iterate over the train dataset\n",
    "      tmp = []\n",
    "      tmp_dist = []\n",
    "      for images, labels in train_dataloader:\n",
    "        # Bring data over the device of choice\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        net.train() \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = net(images)\n",
    "\n",
    "        loss = criterion_clf(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "\n",
    "        if i > 0:\n",
    "          with torch.no_grad():\n",
    "            # loading pre-update parameters for distillation\n",
    "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
    "            outputs_old = prev_net(images)\n",
    "          # distillation loss\n",
    "          loss_dist = criterion_dist(outputs, outputs_old, labels_old)\n",
    "          # Log loss\n",
    "          if current_step % LOG_FREQUENCY == 0:\n",
    "            print('Step {}, Classification Loss {}, Distillation Loss {}'.format(current_step, loss.item(), loss_dist.item())) \n",
    "          tmp_dist.append(loss_dist.item())\n",
    "          loss = loss + loss_dist \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "      # train loss\n",
    "      train_clf_loss.append(np.mean(tmp))\n",
    "      if i > 0:\n",
    "        train_dist_loss.append(np.mean(tmp_dist))\n",
    "      # Step the scheduler\n",
    "      scheduler.step()\n",
    "\n",
    "    # plot train loss\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(np.arange(0, NUM_EPOCHS), train_clf_loss, c='blue', linestyle='-', label='Classification loss')\n",
    "    if i > 0:\n",
    "      ax.plot(np.arange(0, NUM_EPOCHS), train_dist_loss, c='red', linestyle='-', label='Distillation loss')\n",
    "    plt.title('Train loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()  \n",
    "\n",
    "    # reducing previous exemplars\n",
    "    m = K // (10*(i+1))\n",
    "    if i > 0:\n",
    "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
    "\n",
    "    # construct exemplars with current classes\n",
    "    class_means = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m)\n",
    "\n",
    "    net.train(False) # Set Network to evaluation mode\n",
    "\n",
    "    # test\n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "      with torch.no_grad():\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = net(images)\n",
    "        print(\"OUTPUT SHAPE:\", outputs.size())\n",
    "        outputs = nn.functional.softmax(outputs)\n",
    "        print(\"OUTPUT SHAPE:\", outputs.size())\n",
    "\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update Corrects\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    score = running_corrects / float(len(test_subset))\n",
    "    # saving i-batch model parameters (distillation)\n",
    "    torch.save(net, './ICARL/prev_net.pt')\n",
    "    # accuracy of last epoch model\n",
    "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
    "    batches_accuracy.append(score)\n",
    "  \n",
    "  # plot accuracy graph\n",
    "  fig, ax = plt.subplots(figsize=(8, 5))\n",
    "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
    "  plt.title('Accuracy graph vs Number of classes')\n",
    "  plt.tight_layout()\n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  return batches_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6MJ8HSAvuYWq",
    "outputId": "a0f10da7-5b2c-4f2d-9887-09ac6c61310d"
   },
   "outputs": [],
   "source": [
    "scores = icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, distillationLossCE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MLP_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
