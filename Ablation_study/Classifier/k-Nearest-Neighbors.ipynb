{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_QbiFdvCm-O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader, random_split, ConcatDataset\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4Uo3eZsCm-g"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' \n",
    "\n",
    "NUM_CLASSES = 100  \n",
    "\n",
    "BATCH_SIZE = 128     \n",
    "                     \n",
    "K = 2000              \n",
    "LR = 2.0            \n",
    "LR_L2 = 0.25\n",
    "LR_CE = 0.1\n",
    "LR_LFC = 0.1\n",
    "LR_L1 = 0.025        \n",
    "MOMENTUM = 0.9       \n",
    "WEIGHT_DECAY = 1e-5  \n",
    "WEIGHT_DECAY_CE = 5e-4\n",
    "\n",
    "NUM_EPOCHS = 60      \n",
    "MILESTONES = [49, 63]       \n",
    "MILESTONES_L2 = [30, 45]\n",
    "MILESTONES_CE = [50]\n",
    "MILESTONES_LFC = [40]  \n",
    "GAMMA = 0.2          \n",
    "GAMMA_CE = 0.2\n",
    "GAMMA_LFC = 0.2\n",
    "\n",
    "LOG_FREQUENCY = 10\n",
    "SEED = 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOeIquBvCm-s"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(0.5),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
    "                                                           std=[0.2673, 0.2564, 0.2761])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
    "                                                          std=[0.2673, 0.2564, 0.2761])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Bn0aeMZ1Cm-0",
    "outputId": "8b44ceec-337c-4e13-d824-2ba165a3a393"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Clone github repository with data\n",
    "if not os.path.isdir('./ICARL'):\n",
    "  !git clone https://github.com/lbonasera1/ICARL.git\n",
    "\n",
    "from ICARL.ResNet_CIFAR_100.resnet_cifar import resnet32\n",
    "from ICARL.ResNet_CIFAR_100.resnet_cifar_norelu import resnet32_norelu\n",
    "\n",
    "train_dataset = CIFAR100(root='./ICARL/data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = CIFAR100(root='./ICARL/data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_subsets = []\n",
    "test_subsets = []\n",
    "batch_classes = []\n",
    "class_indexes = [None] * NUM_CLASSES  \n",
    "random.seed(SEED)\n",
    "L = list(range(100))\n",
    "random.shuffle(L)\n",
    "\n",
    "for i in range(10):\n",
    "  # random extract 10 classes from 100\n",
    "  batch_classes.append([L.pop() for _ in range(10)])\n",
    "\n",
    "  # search and collect train and exemplars indices for i-batch\n",
    "  train_indices = []\n",
    "  for target in batch_classes[i]:\n",
    "    tmp = []\n",
    "    for idx, val in enumerate(train_dataset.targets):\n",
    "      if val == target:\n",
    "        train_indices.append(idx)\n",
    "        tmp.append(idx)\n",
    "    random.shuffle(tmp)\n",
    "    class_indexes[target] = tmp\n",
    "\n",
    "  random.shuffle(train_indices)\n",
    "  # create subset from indices\n",
    "  subset = Subset(train_dataset, train_indices)\n",
    "  train_subsets.append(subset)\n",
    "\n",
    "  # search and collect train indices for i-batch\n",
    "  test_indices = []\n",
    "  for target in batch_classes[i]:\n",
    "    for idx, val in enumerate(test_dataset.targets):\n",
    "      if target == val:\n",
    "         test_indices.append(idx)\n",
    "\n",
    "  random.shuffle(test_indices)\n",
    "  # create subset from indices\n",
    "  subset = Subset(test_dataset, test_indices)\n",
    "  test_subsets.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhCHY0qNCm-7"
   },
   "outputs": [],
   "source": [
    "### function used for reload the model trained over the previous batch of classe,\n",
    "### in order to calculate the distillation loss\n",
    "def load_checkpoint(filepath):\n",
    "  model = torch.load(filepath)\n",
    "  for parameter in model.parameters():\n",
    "      parameter.requires_grad = False\n",
    "  model.eval()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1NMjK7NCm_B"
   },
   "outputs": [],
   "source": [
    "def constrExemplars(exemplars, classes, class_indexes, model, m):\n",
    "  pdist = nn.PairwiseDistance(p=2)\n",
    "  model.train(False)\n",
    "  model.set_flag(False)\n",
    "  class_means = torch.empty((0, 64)).cuda()\n",
    "  with torch.no_grad():\n",
    "    # compute mean for each class\n",
    "    for c in classes:\n",
    "      indexes = copy.deepcopy(class_indexes[c])\n",
    "      features = torch.empty((0, 64)).cuda()\n",
    "      # image set of class c\n",
    "      subset = Subset(train_dataset, indexes)\n",
    "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
    "      for image, label in dataLoader:\n",
    "        image = image.to(DEVICE)\n",
    "        # extract features\n",
    "        output = model(image)\n",
    "        # L2 normalization of feature vector\n",
    "        output = nn.functional.normalize(output, p=2, dim=1)\n",
    "        features = torch.cat((features, output))\n",
    "      \n",
    "      class_mean = torch.mean(features, 0)\n",
    "      class_mean = nn.functional.normalize(class_mean, p=2, dim=0)\n",
    "      class_mean = class_mean.view(-1, 64)\n",
    "      class_means = torch.cat((class_means, class_mean))\n",
    "      current_features = torch.empty((0, 64)).cuda()\n",
    "      exemplars_indexes = []\n",
    "      for k in range(m):\n",
    "        current_sum = torch.sum(current_features, 0)\n",
    "        current_sum = torch.add(features, current_sum.repeat(features.size(0), 1))\n",
    "        current_mean = current_sum * (1.0/(k+1))\n",
    "        current_mean = nn.functional.normalize(current_mean, p=2, dim=1)\n",
    "        distances = pdist(current_mean, class_mean)\n",
    "        index = torch.argmin(distances).item()   \n",
    "        phi = features[index].view(-1, 64)\n",
    "        # collecting chosen features\n",
    "        current_features = torch.cat((current_features, phi))\n",
    "        # removing chosen features\n",
    "        features = torch.cat((features[:index], features[index+1:]))\n",
    "        exemplars_indexes.append(indexes.pop(index))  \n",
    "      exemplars[c] = exemplars_indexes\n",
    "  model.set_flag(True)\n",
    "  model.train()\n",
    "  return class_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTEQesM4Cm_I"
   },
   "outputs": [],
   "source": [
    "def reduceExemplars(exemplars, classes, m):\n",
    "  for c in classes:\n",
    "    exemplars[c] = exemplars[c][:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzT6ivBGCm_P"
   },
   "outputs": [],
   "source": [
    "def distillationLossCE(outputs, outputs_old, labels_old):\n",
    "  loss = torch.empty([0, 1]).cuda()\n",
    "  weights = torch.nn.functional.softmax(outputs_old, dim=1)\n",
    "  logs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "  results = torch.mul(weights, -logs)\n",
    "  mean = torch.mean(results, dim=0)\n",
    "  for k in labels_old:\n",
    "    loss = torch.cat((loss, mean[k].view(-1, 1)))\n",
    "  loss = torch.sum(loss)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ykEu2KwCm_V"
   },
   "outputs": [],
   "source": [
    "### return a Knn classifier trained over the exemplars\n",
    "def create_classifier(labels_old,labels_new,exemplars,net, num):\n",
    "    KNN = KNeighborsClassifier(n_neighbors = num)\n",
    "    classi = labels_old + labels_new\n",
    "    exemplars_index = [exemplars[i] for i in classi]\n",
    "    exemplars_index = [item for sublist in exemplars_index for item in sublist]\n",
    "    subsetforclassifier = Subset(train_dataset, exemplars_index)\n",
    "    net.set_flag(False)\n",
    "    net.eval()\n",
    "    dlfc = DataLoader(subsetforclassifier, batch_size = 128)\n",
    "    X_train = torch.empty((0,64))\n",
    "    Y_train = torch.empty((0),dtype = torch.long)\n",
    "    for images, labels in dlfc:\n",
    "        images = images.to(DEVICE)\n",
    "        out = net(images)\n",
    "        out = out.to('cpu')\n",
    "        X_train = torch.cat((X_train,out))\n",
    "        Y_train = torch.cat((Y_train, labels))\n",
    "    X_train = X_train.detach().numpy()\n",
    "    Y_train = Y_train.detach().numpy()\n",
    "    print(\"X_train:\",X_train.shape)\n",
    "    print(\"Y_train:\",Y_train.shape)\n",
    "    \n",
    "    KNN.fit(X_train,Y_train)\n",
    "    del images\n",
    "    del out\n",
    "    del labels\n",
    "    torch.cuda.empty_cache()\n",
    "    net.set_flag(False)\n",
    "    return KNN\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GC3VNjRCm_b"
   },
   "outputs": [],
   "source": [
    "\n",
    "def icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, criterion_dist):\n",
    "  net = resnet32()\n",
    "  net = net.to(DEVICE)\n",
    "  cudnn.benchmark\n",
    "  batches_accuracy = []\n",
    "  labels_old = []\n",
    "  test_subList = []\n",
    "  exemplars = [None] * NUM_CLASSES\n",
    "\n",
    "  # iterate over class batches\n",
    "  for i in range(10):\n",
    "    train_clf_loss = []\n",
    "    train_dist_loss = []\n",
    "    criterion_clf = nn.CrossEntropyLoss()\n",
    "    # concatenate test classes\n",
    "    test_subList.append(test_subsets[i])\n",
    "    test_subset = ConcatDataset(test_subList)\n",
    "    # adding exemplars to train subset\n",
    "    train_subset = train_subsets[i]\n",
    "    if i > 0:\n",
    "      # get old labels\n",
    "      for j in batch_classes[i-1]:\n",
    "        labels_old.append(j)\n",
    "      train_subList = []\n",
    "      for k in labels_old:\n",
    "        train_subList = train_subList + exemplars[k]\n",
    "      random.shuffle(train_subList)  \n",
    "      subset = Subset(train_dataset, train_subList)\n",
    "      train_subset = ConcatDataset([train_subset, subset])\n",
    "    # initializate dataloader and variables\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    parameters_to_optimize = net.parameters()\n",
    "    optimizer = optim.SGD(parameters_to_optimize, lr=LR_CE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY_CE)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES_CE, gamma=GAMMA_CE)\n",
    "      \n",
    "    current_step = 0\n",
    "    # Start iterating over the epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
    "      # Iterate over the train dataset\n",
    "      tmp = []\n",
    "      tmp_dist = []\n",
    "      for images, labels in train_dataloader:\n",
    "        # Bring data over the device of choice\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        net.train() \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = net(images)\n",
    "\n",
    "        loss = criterion_clf(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "\n",
    "        if i > 0:\n",
    "          with torch.no_grad():\n",
    "            # loading pre-update parameters for distillation\n",
    "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
    "            outputs_old = prev_net(images)\n",
    "          # distillation loss\n",
    "          loss_dist = criterion_dist(outputs, outputs_old, labels_old)\n",
    "          # Log loss\n",
    "          if current_step % LOG_FREQUENCY == 0:\n",
    "            print('Step {}, Classification Loss {}, Distillation Loss {}'.format(current_step, loss.item(), loss_dist.item())) \n",
    "          tmp_dist.append(loss_dist.item())\n",
    "          loss = loss + loss_dist \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "      train_clf_loss.append(np.mean(tmp))\n",
    "      if i > 0:\n",
    "        train_dist_loss.append(np.mean(tmp_dist))\n",
    "      scheduler.step()\n",
    "\n",
    "    # plot train loss\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(np.arange(0, NUM_EPOCHS), train_clf_loss, c='blue', linestyle='-', label='Classification loss')\n",
    "    if i > 0:\n",
    "      ax.plot(np.arange(0, NUM_EPOCHS), train_dist_loss, c='red', linestyle='-', label='Distillation loss')\n",
    "    plt.title('Train loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()  \n",
    "\n",
    "    # reducing previous exemplars\n",
    "    m = K // (10*(i+1))\n",
    "    if i > 0:\n",
    "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
    "\n",
    "    # construct exemplars with current classes\n",
    "    class_means = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m)\n",
    "\n",
    "    net.set_flag(False)\n",
    "    net.train(False)\n",
    "    \n",
    "    classifier = create_classifier(labels_old,batch_classes[i],exemplars,net,m//2)\n",
    "\n",
    "    # test\n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "      with torch.no_grad():\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        out = net(images)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        pred = classifier.predict(out)\n",
    "        for h in range(out.shape[0]):\n",
    "            if labels[h].item() == pred[h]:\n",
    "                running_corrects = running_corrects + 1\n",
    "        \n",
    "    # Calculate Accuracy\n",
    "    score = running_corrects / float(len(test_subset))\n",
    "    net.set_flag(True)\n",
    "    # saving i-batch model parameters (distillation)\n",
    "    torch.save(net, './ICARL/prev_net.pt')\n",
    "    # accuracy of last epoch model\n",
    "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
    "    batches_accuracy.append(score)\n",
    "  \n",
    "  # plot accuracy graph\n",
    "  fig, ax = plt.subplots(figsize=(8, 5))\n",
    "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
    "  plt.title('Accuracy graph vs Number of classes')\n",
    "  plt.tight_layout()\n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  return batches_accuracy, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G5atEO4LCm_h",
    "outputId": "337974ff-d97e-4111-e2e8-c17747a0a988"
   },
   "outputs": [],
   "source": [
    "scores, classificatore = icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, distillationLossCE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "knnclassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
