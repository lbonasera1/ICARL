{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICARL_ABLATION__COSINE_(1) (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWowydr8FQa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, random_split, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmUFqcrgFX0A",
        "colab_type": "text"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj4MHx74FThJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = 100  \n",
        "\n",
        "BATCH_SIZE = 128     \n",
        "                     \n",
        "K = 2000              \n",
        "LR = 2.0            \n",
        "LR_L2 = 0.25\n",
        "LR_CE = 0.1\n",
        "LR_LFC = 0.01\n",
        "LR_L1 = 0.1        \n",
        "MOMENTUM = 0.9       \n",
        "WEIGHT_DECAY = 1e-5  \n",
        "WEIGHT_DECAY_CE = 5e-4\n",
        "\n",
        "NUM_EPOCHS = 60      \n",
        "MILESTONES = [49, 63] \n",
        "MILESTONES_L2 = [30, 45]\n",
        "MILESTONES_CE = [50]\n",
        "MILESTONES_LFC = [50]\n",
        "GAMMA = 0.2       \n",
        "GAMMA_CE = 0.2\n",
        "GAMMA_LFC = 0.2\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "SEED = 1993"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVmu0DB2Ikvy",
        "colab_type": "text"
      },
      "source": [
        "**Transform**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFtCfK54Ikil",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(0.5),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                                                           std=[0.2673, 0.2564, 0.2761])])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                                                          std=[0.2673, 0.2564, 0.2761])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieNHuVP3Fev7",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hvmV4fYFV5a",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('./ICARL'):\n",
        "  !git clone https://github.com/lbonasera1/ICARL.git\n",
        "\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar import resnet32\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar_norelu import resnet32_norelu\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar_cosine import resnet32_cosine\n",
        "\n",
        "train_dataset = CIFAR100(root='./ICARL/data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(root='./ICARL/data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_subsets = []\n",
        "test_subsets = []\n",
        "batch_classes = []\n",
        "class_indexes = [None] * NUM_CLASSES  \n",
        "random.seed(SEED)\n",
        "L = list(range(100))\n",
        "random.shuffle(L)\n",
        "\n",
        "for i in range(10):\n",
        "  # random extract 10 classes from 100\n",
        "  batch_classes.append([L.pop() for _ in range(10)])\n",
        "\n",
        "  # search and collect train and exemplars indices for i-batch\n",
        "  train_indices = []\n",
        "  for target in batch_classes[i]:\n",
        "    tmp = []\n",
        "    for idx, val in enumerate(train_dataset.targets):\n",
        "      if val == target:\n",
        "        train_indices.append(idx)\n",
        "        tmp.append(idx)\n",
        "    random.shuffle(tmp)\n",
        "    class_indexes[target] = tmp\n",
        "\n",
        "  random.shuffle(train_indices)\n",
        "  # create subset from indices\n",
        "  subset = Subset(train_dataset, train_indices)\n",
        "  train_subsets.append(subset)\n",
        "\n",
        "  # search and collect train indices for i-batch\n",
        "  test_indices = []\n",
        "  for target in batch_classes[i]:\n",
        "    for idx, val in enumerate(test_dataset.targets):\n",
        "      if target == val:\n",
        "         test_indices.append(idx)\n",
        "\n",
        "  random.shuffle(test_indices)\n",
        "  # create subset from indices\n",
        "  subset = Subset(test_dataset, test_indices)\n",
        "  test_subsets.append(subset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2969SygPGE0",
        "colab_type": "text"
      },
      "source": [
        "**Create loss target**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOYwa2ezPF1T",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def lossTarget(labels, output_old=None, labels_old=None):\n",
        "  tensor = torch.zeros((labels.size(0), NUM_CLASSES), device=\"cuda:0\")\n",
        "  for i in range(labels.size(0)):\n",
        "    # one-hot for new classes\n",
        "    tensor.data[i][labels[i]] = 1\n",
        "    # distillation from previous classes\n",
        "    if output_old is not None or labels_old is not None:\n",
        "      for j in labels_old:\n",
        "        tensor.data[i][j] = output_old.data[i][j]          \n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRPAhb0KNeiH",
        "colab_type": "text"
      },
      "source": [
        "**Loading previous model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB2xtBZSNb6y",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "  model = torch.load(filepath)\n",
        "  for parameter in model.parameters():\n",
        "      parameter.requires_grad = False\n",
        "  model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_6ceSWeMRvB",
        "colab_type": "text"
      },
      "source": [
        "**Construct Exemplars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaKeejAPMQwX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def constrExemplars(exemplars, classes, class_indexes, model, m):\n",
        "  pdist = nn.PairwiseDistance(p=2)\n",
        "  model.train(False)\n",
        "  model.set_flag(False)\n",
        "  class_means = torch.empty((0, 64)).cuda()\n",
        "  with torch.no_grad():\n",
        "    # compute mean for each class\n",
        "    for c in classes:\n",
        "      indexes = copy.deepcopy(class_indexes[c])\n",
        "      features = torch.empty((0, 64)).cuda()\n",
        "      # image set of class c\n",
        "      subset = Subset(train_dataset, indexes)\n",
        "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "      for image, label in dataLoader:\n",
        "        image = image.to(DEVICE)\n",
        "        # extract features\n",
        "        output = model(image)\n",
        "        # L2 normalization of feature vector\n",
        "        output = nn.functional.normalize(output, p=2, dim=1)\n",
        "        features = torch.cat((features, output))\n",
        "      \n",
        "      class_mean = torch.mean(features, 0)\n",
        "      class_mean = nn.functional.normalize(class_mean, p=2, dim=0)\n",
        "      class_mean = class_mean.view(-1, 64)\n",
        "      class_means = torch.cat((class_means, class_mean))\n",
        "      current_features = torch.empty((0, 64)).cuda()\n",
        "      exemplars_indexes = []\n",
        "      for k in range(m):\n",
        "        current_sum = torch.sum(current_features, 0)\n",
        "        current_sum = torch.add(features, current_sum.repeat(features.size(0), 1))\n",
        "        current_mean = current_sum * (1.0/(k+1))\n",
        "        current_mean = nn.functional.normalize(current_mean, p=2, dim=1)\n",
        "        distances = pdist(current_mean, class_mean)\n",
        "        index = torch.argmin(distances).item()   \n",
        "        phi = features[index].view(-1, 64)\n",
        "        # collecting chosen features\n",
        "        current_features = torch.cat((current_features, phi))\n",
        "        # removing chosen features\n",
        "        features = torch.cat((features[:index], features[index+1:]))\n",
        "        exemplars_indexes.append(indexes.pop(index))  \n",
        "      exemplars[c] = exemplars_indexes\n",
        "  model.set_flag(True)\n",
        "  model.train()\n",
        "  return class_means\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTrzbBmSvvMd",
        "colab_type": "text"
      },
      "source": [
        "**Reduce Exemplars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1YO1b99vu9R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def reduceExemplars(exemplars, classes, m):\n",
        "  for c in classes:\n",
        "    exemplars[c] = exemplars[c][:m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Yo8MAVRUIU",
        "colab_type": "text"
      },
      "source": [
        "**NCM Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_vtXFU2RQXv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def classifierNCM(input_image, exemplars, labels_old, labels_new, class_means, model):\n",
        "  pdist = nn.PairwiseDistance(p=2)\n",
        "  model.train(False)\n",
        "  model.set_flag(False)\n",
        "  with torch.no_grad():\n",
        "    tensor = torch.zeros((input_image.size(0), NUM_CLASSES), device=\"cuda:0\")\n",
        "    exemplars_mean = torch.empty((0, 64)).cuda()\n",
        "    for c in labels_old:\n",
        "      subset = Subset(train_dataset, exemplars[c])\n",
        "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "      mean = torch.empty((0, 64)).cuda()\n",
        "      for image, label in dataLoader:\n",
        "        image = image.to(DEVICE)\n",
        "        output = model(image)\n",
        "        # L2 normalization of feature vector\n",
        "        output = nn.functional.normalize(output, p=2, dim=1)\n",
        "        mean = torch.cat((mean, output))\n",
        "      mean = torch.mean(mean, 0)\n",
        "      mean = nn.functional.normalize(mean, p=2, dim=0)\n",
        "      mean = mean.view(-1, 64)\n",
        "      exemplars_mean = torch.cat((exemplars_mean, mean))\n",
        "\n",
        "    exemplars_mean = torch.cat((exemplars_mean, class_means))\n",
        "    classes = labels_old + labels_new\n",
        "\n",
        "    output = model(input_image)\n",
        "    output = nn.functional.normalize(output, p=2, dim=1)\n",
        "    for n in range((output.size(0))):\n",
        "      image = output[n]\n",
        "      distances = pdist(image, exemplars_mean)\n",
        "      index = torch.argmin(distances).item()\n",
        "      index = classes[index]\n",
        "      tensor[n][index] = 1\n",
        "\n",
        "  model.set_flag(True)\n",
        "  model.train()\n",
        "  return tensor  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaCzWr0P_mI",
        "colab_type": "text"
      },
      "source": [
        "**Lp loss target**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdhAaSqXP_NI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def l2Loss(outputs, outputs_old, labels, labels_old):\n",
        "  tensor = outputs.clone().detach()\n",
        "  tensor.requires_grad = True\n",
        "  for i in range(labels.size(0)):\n",
        "    for k in labels_old:\n",
        "      tensor.data[i][k] = outputs_old.data[i][k]\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L29g0NzqCYEc",
        "colab_type": "text"
      },
      "source": [
        "**CE Distillation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX_ZUIo2CXx7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def distillationLossCE(outputs, outputs_old, labels_old):\n",
        "  loss = torch.empty([0, 1]).cuda()\n",
        "  weights = torch.nn.functional.softmax(outputs_old, dim=1)\n",
        "  logs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
        "  results = torch.mul(weights, -logs)\n",
        "  mean = torch.mean(results, dim=0)\n",
        "  for k in labels_old:\n",
        "    loss = torch.cat((loss, mean[k].view(-1, 1)))\n",
        "  loss = torch.sum(loss)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7kFWZIaow5O",
        "colab_type": "text"
      },
      "source": [
        "**Ablation study CE + CE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "WjbcNqeyov4H",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, criterion_dist):\n",
        "  net = resnet32()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "    train_clf_loss = []\n",
        "    train_dist_loss = []\n",
        "    criterion_clf = nn.CrossEntropyLoss()\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      # get old labels\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "        train_subList = train_subList + exemplars[k]\n",
        "      random.shuffle(train_subList)  \n",
        "      subset = Subset(train_dataset, train_subList)\n",
        "      train_subset = ConcatDataset([train_subset, subset])\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR_CE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY_CE)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES_CE, gamma=GAMMA_CE)\n",
        "      \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
        "      # Iterate over the train dataset\n",
        "      tmp = []\n",
        "      tmp_dist = []\n",
        "      for images, labels in train_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train() # Sets module in training mode\n",
        "        optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        loss = criterion_clf(outputs, labels)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        if i > 0:\n",
        "          with torch.no_grad():\n",
        "            # loading pre-update parameters for distillation\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            outputs_old = prev_net(images)\n",
        "          # distillation loss\n",
        "          loss_dist = criterion_dist(outputs, outputs_old, labels_old)\n",
        "          # Log loss\n",
        "          if current_step % LOG_FREQUENCY == 0:\n",
        "            print('Step {}, Classification Loss {}, Distillation Loss {}'.format(current_step, loss.item(), loss_dist.item())) \n",
        "          tmp_dist.append(loss_dist.item())\n",
        "          loss = loss + loss_dist \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # train loss\n",
        "      train_clf_loss.append(np.mean(tmp))\n",
        "      if i > 0:\n",
        "        train_dist_loss.append(np.mean(tmp_dist))\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "    # plot train loss\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_clf_loss, c='blue', linestyle='-', label='Classification loss')\n",
        "    if i > 0:\n",
        "      ax.plot(np.arange(0, NUM_EPOCHS), train_dist_loss, c='red', linestyle='-', label='Distillation loss')\n",
        "    plt.title('Train loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()  \n",
        "\n",
        "    # reducing previous exemplars\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
        "\n",
        "    # construct exemplars with current classes\n",
        "    class_means = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m)\n",
        "\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    # test\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      with torch.no_grad():\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = classifierNCM(images, exemplars, labels_old, batch_classes[i], class_means, net)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    score = running_corrects / float(len(test_subset))\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "    # accuracy of last epoch model\n",
        "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
        "    batches_accuracy.append(score)\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1zZT83KMAEC",
        "colab_type": "text"
      },
      "source": [
        "**Less Forget Contraint loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBlvC-Q1L_yx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def lfcLoss(outputs, outputs_old, n):\n",
        "  old = nn.functional.normalize(outputs_old, p=2, dim=1)\n",
        "  current = nn.functional.normalize(outputs, p=2, dim=1)\n",
        "  dot = torch.bmm(old.view(n, 1, 64), current.view(n, 64, 1)).view(-1, n)\n",
        "  dot = torch.mean(dot, 1)\n",
        "  # dot = torch.nn.functional.cosine_similarity(old, current)\n",
        "  return 1 - dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6N68PiQLHjN",
        "colab_type": "text"
      },
      "source": [
        "**Ablation study CE + LFC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqHgE7YMLHUd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "def icarl_ablation_ce_lcf(train_subsets, test_subsets, batch_classes, class_indexes, criterion_dist):\n",
        "  net = resnet32_norelu()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "    train_clf_loss = []\n",
        "    train_dist_loss = []\n",
        "    criterion_clf = nn.CrossEntropyLoss()\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      # get old labels\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "        train_subList = train_subList + exemplars[k]\n",
        "      random.shuffle(train_subList)  \n",
        "      subset = Subset(train_dataset, train_subList)\n",
        "      train_subset = ConcatDataset([train_subset, subset])\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR_LFC, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY_CE)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES_LFC, gamma=GAMMA_LFC)\n",
        "      \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
        "      # Iterate over the train dataset\n",
        "      tmp = []\n",
        "      tmp_dist = []\n",
        "      for images, labels in train_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train() # Sets module in training mode\n",
        "        optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        loss = criterion_clf(outputs, labels)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        if i > 0:\n",
        "          with torch.no_grad():\n",
        "            # loading pre-update parameters for distillation\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            prev_net.set_flag(False)\n",
        "            features_old = prev_net(images)\n",
        "            net.set_flag(False)\n",
        "            features_current = net(images)\n",
        "            net.set_flag(True)\n",
        "          # distillation loss\n",
        "          loss_dist = criterion_dist(features_current, features_old, labels.size(0))\n",
        "          # Log loss\n",
        "          if current_step % LOG_FREQUENCY == 0:\n",
        "            print('Step {}, Classification Loss {}, Distillation Loss {}'.format(current_step, loss.item(), loss_dist.item())) \n",
        "          tmp_dist.append(loss_dist.item())\n",
        "          loss = loss + loss_dist \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # train loss\n",
        "      train_clf_loss.append(np.mean(tmp))\n",
        "      if i > 0:\n",
        "        train_dist_loss.append(np.mean(tmp_dist))\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "    # plot train loss\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_clf_loss, c='blue', linestyle='-', label='Classification loss')\n",
        "    if i > 0:\n",
        "      ax.plot(np.arange(0, NUM_EPOCHS), train_dist_loss, c='red', linestyle='-', label='Distillation loss')\n",
        "    plt.title('Train loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()  \n",
        "\n",
        "    # reducing previous exemplars\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
        "\n",
        "    # construct exemplars with current classes\n",
        "    class_means = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m)\n",
        "\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    # test\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      with torch.no_grad():\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = classifierNCM(images, exemplars, labels_old, batch_classes[i], class_means, net)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    score = running_corrects / float(len(test_subset))\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "    # accuracy of last epoch model\n",
        "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
        "    batches_accuracy.append(score)\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38igRzF6nCaD",
        "colab_type": "text"
      },
      "source": [
        "**Lp Distillation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-mUpoianCN1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def l1Loss(outputs, outputs_old, p):\n",
        "  pdist = nn.PairwiseDistance(p=p)\n",
        "  old = nn.functional.normalize(outputs_old, p=2, dim=1)\n",
        "  current = nn.functional.normalize(outputs, p=2, dim=1)\n",
        "  loss = pdist(old, current)\n",
        "  loss = torch.mean(loss, 0)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aB9iwdyqqbY",
        "colab_type": "text"
      },
      "source": [
        "**Ablation study CE + L1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "BC1QzS65qpLP",
        "colab": {}
      },
      "source": [
        "def icarl_ablation_ce_l1(train_subsets, test_subsets, batch_classes, class_indexes, criterion_dist):\n",
        "  net = resnet32()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "    train_clf_loss = []\n",
        "    train_dist_loss = []\n",
        "    criterion_clf = nn.CrossEntropyLoss()\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      # get old labels\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "        train_subList = train_subList + exemplars[k]\n",
        "      random.shuffle(train_subList)  \n",
        "      subset = Subset(train_dataset, train_subList)\n",
        "      train_subset = ConcatDataset([train_subset, subset])\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR_L1, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY_CE)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES_LFC, gamma=GAMMA_LFC)\n",
        "      \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
        "      # Iterate over the train dataset\n",
        "      tmp = []\n",
        "      tmp_dist = []\n",
        "      for images, labels in train_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train() # Sets module in training mode\n",
        "        optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        loss = criterion_clf(outputs, labels)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        if i > 0:\n",
        "          with torch.no_grad():\n",
        "            # loading pre-update parameters for distillation\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            prev_net.set_flag(False)\n",
        "            features_old = prev_net(images)\n",
        "            net.set_flag(False)\n",
        "            features_current = net(images)\n",
        "            net.set_flag(True)\n",
        "          # distillation loss\n",
        "          loss_dist = criterion_dist(features_current, features_old, 1)\n",
        "          # Log loss\n",
        "          if current_step % LOG_FREQUENCY == 0:\n",
        "            print('Step {}, Classification Loss {}, Distillation Loss {}'.format(current_step, loss.item(), loss_dist.item())) \n",
        "          tmp_dist.append(loss_dist.item())\n",
        "          loss = loss + loss_dist \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # train loss\n",
        "      train_clf_loss.append(np.mean(tmp))\n",
        "      if i > 0:\n",
        "        train_dist_loss.append(np.mean(tmp_dist))\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "    # plot train loss\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_clf_loss, c='blue', linestyle='-', label='Classification loss')\n",
        "    if i > 0:\n",
        "      ax.plot(np.arange(0, NUM_EPOCHS), train_dist_loss, c='red', linestyle='-', label='Distillation loss')\n",
        "    plt.title('Train loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()  \n",
        "\n",
        "    # reducing previous exemplars\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
        "\n",
        "    # construct exemplars with current classes\n",
        "    class_means = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m)\n",
        "\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    # test\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      with torch.no_grad():\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = classifierNCM(images, exemplars, labels_old, batch_classes[i], class_means, net)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    score = running_corrects / float(len(test_subset))\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "    # accuracy of last epoch model\n",
        "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
        "    batches_accuracy.append(score)\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZq0DUNeOs7G",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJnfYZh0YmzC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Ablation study\n",
        "\n",
        "# CE + CE\n",
        "# scores = icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, distillationLossCE)\n",
        "\n",
        "# CE + LFC\n",
        "# scores = icarl_ablation_ce_lcf(train_subsets, test_subsets, batch_classes, class_indexes, lfcLoss)\n",
        "\n",
        "# CE + L1\n",
        "# scores = icarl_ablation_ce_l1(train_subsets, test_subsets, batch_classes, class_indexes, l1Loss)\n",
        "\n",
        "# print(scores)\n",
        "# print(\"Average multi-class accuracy: {}\".format(np.mean(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}