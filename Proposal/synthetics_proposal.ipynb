{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "propostafinale_COVARIANCE_correzione.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_QbiFdvCm-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, random_split, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Uo3eZsCm-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = 100  \n",
        "\n",
        "BATCH_SIZE = 128    \n",
        "K = 2000           \n",
        "LR = 2.0            \n",
        "LR_L2 = 0.25\n",
        "LR_CE = 0.1\n",
        "LR_LFC = 0.1\n",
        "LR_L1 = 0.1        \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-5 \n",
        "WEIGHT_DECAY_CE = 5e-4\n",
        "\n",
        "NUM_EPOCHS = 70    \n",
        "MILESTONES = [49, 63]  \n",
        "GAMMA = 0.2      \n",
        "GAMMA_CE = 0.1\n",
        "\n",
        "NN = 50\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "SEED = 1992"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOeIquBvCm-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(0.5),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                                                           std=[0.2673, 0.2564, 0.2761])])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                                                          std=[0.2673, 0.2564, 0.2761])])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn0aeMZ1Cm-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./ICARL'):\n",
        "  !git clone https://github.com/lbonasera1/ICARL.git\n",
        "\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar import resnet32\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar_norelu import resnet32_norelu\n",
        "\n",
        "train_dataset = CIFAR100(root='./ICARL/data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(root='./ICARL/data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_subsets = []\n",
        "test_subsets = []\n",
        "batch_classes = []\n",
        "class_indexes = [None] * NUM_CLASSES  \n",
        "random.seed(SEED)\n",
        "L = list(range(100))\n",
        "random.shuffle(L)\n",
        "\n",
        "for i in range(10):\n",
        "  # random extract 10 classes from 100\n",
        "  batch_classes.append([L.pop() for _ in range(10)])\n",
        "\n",
        "  # search and collect train and exemplars indices for i-batch\n",
        "  train_indices = []\n",
        "  for target in batch_classes[i]:\n",
        "    tmp = []\n",
        "    for idx, val in enumerate(train_dataset.targets):\n",
        "      if val == target:\n",
        "        train_indices.append(idx)\n",
        "        tmp.append(idx)\n",
        "    random.shuffle(tmp)\n",
        "    class_indexes[target] = tmp\n",
        "\n",
        "  random.shuffle(train_indices)\n",
        "  # create subset from indices\n",
        "  subset = Subset(train_dataset, train_indices)\n",
        "  train_subsets.append(subset)\n",
        "\n",
        "  # search and collect train indices for i-batch\n",
        "  test_indices = []\n",
        "  for target in batch_classes[i]:\n",
        "    for idx, val in enumerate(test_dataset.targets):\n",
        "      if target == val:\n",
        "         test_indices.append(idx)\n",
        "\n",
        "  random.shuffle(test_indices)\n",
        "  # create subset from indices\n",
        "  subset = Subset(test_dataset, test_indices)\n",
        "  test_subsets.append(subset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhCHY0qNCm-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "  model = torch.load(filepath)\n",
        "  for parameter in model.parameters():\n",
        "      parameter.requires_grad = False\n",
        "  model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1NMjK7NCm_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def constrExemplars(exemplars, classes, class_indexes, model, m, class_stds):\n",
        "  pdist = nn.PairwiseDistance(p=2)\n",
        "  model.train(False)\n",
        "  model.set_flag(False)\n",
        "  class_means = torch.empty((0, 64)).cuda()\n",
        "  class_meansnonorm = torch.empty((0, 64)).cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # compute mean for each class\n",
        "    for c in classes:\n",
        "      indexes = copy.deepcopy(class_indexes[c])\n",
        "      features = torch.empty((0, 64)).cuda()\n",
        "      # image set of class c\n",
        "      subset = Subset(train_dataset, indexes)\n",
        "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "      for image, label in dataLoader:\n",
        "        image = image.to(DEVICE)\n",
        "        # extract features\n",
        "        output = model(image)\n",
        "        # L2 normalization of feature vector\n",
        "        output = nn.functional.normalize(output, p=2, dim=1)\n",
        "        features = torch.cat((features, output))\n",
        "      \n",
        "      class_mean = torch.mean(features, 0)\n",
        "      cpu_features = copy.deepcopy(features)\n",
        "      cpu_features = cpu_features.to('cpu').detach().numpy()\n",
        "      covariance = np.cov(cpu_features, rowvar = False)\n",
        "      covariance = torch.from_numpy(covariance).cuda()\n",
        "      covariance = covariance.type(torch.DoubleTensor)\n",
        "\n",
        "      class_meannonorm = copy.deepcopy(class_mean)\n",
        "      class_mean = nn.functional.normalize(class_mean, p=2, dim=0)\n",
        "      class_mean = class_mean.view(-1, 64)\n",
        "      class_meannonorm = class_meannonorm.view(-1, 64)\n",
        "      class_means = torch.cat((class_means, class_mean))\n",
        "      class_meansnonorm = torch.cat((class_meansnonorm, class_meannonorm))\n",
        "      class_stds[c] = covariance\n",
        "      current_features = torch.empty((0, 64)).cuda()\n",
        "      exemplars_indexes = []\n",
        "      for k in range(m):\n",
        "        current_sum = torch.sum(current_features, 0)\n",
        "        current_sum = torch.add(features, current_sum.repeat(features.size(0), 1))\n",
        "        current_mean = current_sum * (1.0/(k+1))\n",
        "        current_mean = nn.functional.normalize(current_mean, p=2, dim=1)\n",
        "        distances = pdist(current_mean, class_mean)\n",
        "        index = torch.argmin(distances).item()   \n",
        "        phi = features[index].view(-1, 64)\n",
        "        # collecting chosen features\n",
        "        current_features = torch.cat((current_features, phi))\n",
        "        # removing chosen features\n",
        "        features = torch.cat((features[:index], features[index+1:]))\n",
        "        exemplars_indexes.append(indexes.pop(index))  \n",
        "      exemplars[c] = exemplars_indexes\n",
        "  model.set_flag(True)\n",
        "  model.train()\n",
        "  return class_meansnonorm, class_stds\n",
        "  # return class_means, class_stds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTEQesM4Cm_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduceExemplars(exemplars, classes, m):\n",
        "  for c in classes:\n",
        "    exemplars[c] = exemplars[c][:m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzT6ivBGCm_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distillationLossCE(outputs, outputs_old, labels_old):\n",
        "  loss = torch.empty([0, 1]).cuda()\n",
        "  weights = torch.nn.functional.softmax(outputs_old, dim=1)\n",
        "  logs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
        "  results = torch.mul(weights, -logs)\n",
        "  mean = torch.mean(results, dim=0)\n",
        "  for k in labels_old:\n",
        "    loss = torch.cat((loss, mean[k].view(-1, 1)))\n",
        "  loss = torch.sum(loss)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piBEgT5_poXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lfcLoss(outputs, outputs_old, labels_old=None):\n",
        "  old = nn.functional.normalize(outputs_old, p=2, dim=1)\n",
        "  current = nn.functional.normalize(outputs, p=2, dim=1)\n",
        "  loss = torch.nn.functional.cosine_similarity(old, current)\n",
        "  loss = torch.mean(loss, 0)\n",
        "  return 1 - loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ykEu2KwCm_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_classifier(labels_old,labels_new,class_means,exemplars,net,class_stds, m):\n",
        "    KNN = KNeighborsClassifier(n_neighbors = NN)\n",
        "    ## construct X_train Y_train\n",
        "    classi = labels_old + labels_new\n",
        "    exemplars_index = [exemplars[i] for i in classi]\n",
        "    exemplars_index = [item for sublist in exemplars_index for item in sublist]\n",
        "    subsetforclassifier = Subset(train_dataset, exemplars_index)\n",
        "    net.set_flag(False)\n",
        "    net.eval()\n",
        "    dlfc = DataLoader(subsetforclassifier, batch_size = 128)\n",
        "    X_train = torch.empty((0, 64))\n",
        "    X_formean = torch.empty((0, 64))\n",
        "    Y_train = torch.empty((0), dtype = torch.long)\n",
        "    print(\"Loading exemplars\")\n",
        "    for images, labels in dlfc:\n",
        "        images = images.to(DEVICE)\n",
        "        out = net(images)\n",
        "        out = nn.functional.normalize(out, p=2, dim=1)\n",
        "        out = out.to('cpu')\n",
        "        X_train = torch.cat((X_train, out))\n",
        "        Y_train = torch.cat((Y_train, labels))\n",
        "        del out\n",
        "        del images\n",
        "        del labels\n",
        "        torch.cuda.empty_cache()\n",
        "    del dlfc\n",
        "    del subsetforclassifier\n",
        "    torch.cuda.empty_cache()\n",
        "        \n",
        "    Y_train = Y_train.detach().numpy()\n",
        "    for i, c in enumerate(labels_new):\n",
        "      numberofdata = 200 - len(exemplars[c])\n",
        "      if numberofdata == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "        break\n",
        "      mean = class_means[i]\n",
        "      mean = mean.to('cpu')\n",
        "      mean = mean.type(torch.DoubleTensor)\n",
        "      cov = class_stds[c]\n",
        "      cov = cov.to('cpu')\n",
        "      for _ in range(numberofdata):\n",
        "        out1 = np.random.multivariate_normal(mean, cov)\n",
        "        out1 = torch.from_numpy(out1)\n",
        "        out1 = torch.abs(out1)\n",
        "        out1 = out1.view(-1, 64)\n",
        "        out1 = out1.type(torch.FloatTensor)\n",
        "        X_train = torch.cat((X_train, out1))\n",
        "        Y_train = np.hstack((Y_train, c))\n",
        "        del out1\n",
        "        torch.cuda.empty_cache()\n",
        "      del mean\n",
        "      del cov\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    ###SYNTHETICS FOR OLD CLASSES\n",
        "    X_train = X_train.detach().numpy()\n",
        "    for c in labels_old:\n",
        "      net.set_flag(False)\n",
        "      net.eval()\n",
        "      indiciclasse = np.where(Y_train == c)\n",
        "      datiesemplari = X_train[indiciclasse]\n",
        "      meanofclass = np.mean(datiesemplari, axis = 0)\n",
        "      covofclass = class_stds[c]\n",
        "      covofclass = covofclass.to('cpu')\n",
        "      covofclass = covofclass.detach().numpy()\n",
        "      numberofdata = 200 - len(exemplars[c])\n",
        "      for _ in range(numberofdata):\n",
        "        out1 = np.random.multivariate_normal(meanofclass, covofclass)\n",
        "        out1 = np.absolute(out1)\n",
        "        out1 = np.reshape(out1, (1, 64))\n",
        "        X_train = np.vstack((X_train, out1))\n",
        "        del out1\n",
        "        Y_train = np.hstack((Y_train, c))\n",
        "      del meanofclass\n",
        "      del covofclass\n",
        "      torch.cuda.empty_cache()\n",
        "      \n",
        "    KNN.fit(X_train, Y_train)\n",
        "    torch.cuda.empty_cache()\n",
        "    net.set_flag(False)\n",
        "    return KNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GC3VNjRCm_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, criterion_dist):\n",
        "  net = resnet32()\n",
        "  # net = resnet32_norelu()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "  class_stds = [None] * NUM_CLASSES\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "    train_clf_loss = []\n",
        "    train_dist_loss = []\n",
        "    criterion_clf = nn.CrossEntropyLoss()\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      # get old labels\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "        train_subList = train_subList + exemplars[k]\n",
        "      random.shuffle(train_subList)  \n",
        "      subset = Subset(train_dataset, train_subList)\n",
        "      train_subset = ConcatDataset([train_subset, subset])\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR_CE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY_CE)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES_CE, gamma=GAMMA_CE)\n",
        "      \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
        "      # Iterate over the train dataset\n",
        "      tmp = []\n",
        "      tmp_dist = []\n",
        "      for images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train()\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        loss = criterion_clf(outputs, labels)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        if i > 0:\n",
        "          with torch.no_grad():\n",
        "            # loading pre-update parameters for distillation\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            outputs_old = prev_net(images)\n",
        "          # distillation loss\n",
        "          loss_dist = criterion_dist(outputs, outputs_old, labels_old)\n",
        "          # Log loss\n",
        "          if current_step % LOG_FREQUENCY == 0:\n",
        "            print('Step {}, Classification Loss {}, Distillation Loss {}'.format(current_step, loss.item(), loss_dist.item())) \n",
        "          tmp_dist.append(loss_dist.item())\n",
        "          loss = loss + loss_dist \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del images\n",
        "        del labels\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # train loss\n",
        "      train_clf_loss.append(np.mean(tmp))\n",
        "      if i > 0:\n",
        "        train_dist_loss.append(np.mean(tmp_dist))\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "    # plot train loss\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_clf_loss, c='blue', linestyle='-', label='Classification loss')\n",
        "    if i > 0:\n",
        "      ax.plot(np.arange(0, NUM_EPOCHS), train_dist_loss, c='red', linestyle='-', label='Distillation loss')\n",
        "    plt.title('Train loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()  \n",
        "\n",
        "    # reducing previous exemplars\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
        "\n",
        "    # construct exemplars with current classes\n",
        "    class_means, class_stds = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m, class_stds)\n",
        "\n",
        "    net.set_flag(False)\n",
        "    net.train(False)\n",
        "    \n",
        "    classifier = create_classifier(labels_old, batch_classes[i], class_means, exemplars, net, class_stds, m)\n",
        "\n",
        "    # test\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      with torch.no_grad():\n",
        "        images = images.to(DEVICE)\n",
        "        \n",
        "        out = net(images)\n",
        "        out = nn.functional.normalize(out, p=2, dim=1)\n",
        "        out = out.cpu().detach().numpy()\n",
        "        pred = classifier.predict(out)\n",
        "        for h in range(out.shape[0]):\n",
        "            if labels[h].item() == pred[h]:\n",
        "                running_corrects = running_corrects + 1\n",
        "        \n",
        "    # Calculate Accuracy\n",
        "    score = running_corrects / float(len(test_subset))\n",
        "    net.set_flag(True)\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "    # accuracy of last epoch model\n",
        "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
        "    batches_accuracy.append(score)\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return batches_accuracy, classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5atEO4LCm_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores, classifier = icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, distillationLossCE)\n",
        "# scores = icarl_ablation_ce_ce(train_subsets, test_subsets, batch_classes, class_indexes, lfcLoss)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhxK6AD4Jyhy",
        "colab_type": "text"
      },
      "source": [
        "**Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBp0q0zqDwOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "net.eval()\n",
        "net.set_flag(False)\n",
        "test_subList = []\n",
        "for i in range(10):\n",
        "    test_subList.append(test_subsets[i])\n",
        "test_subset = ConcatDataset(test_subList)\n",
        "test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "pred = torch.empty((0),dtype=torch.long)\n",
        "pred = pred.to(DEVICE)\n",
        "label = torch.empty((0),dtype=torch.long)\n",
        "label = label.to(DEVICE)\n",
        "net.eval()\n",
        "running_corrects = 0\n",
        "for images, labels in test_dataloader:\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    out = net(images)\n",
        "    out = out.cpu().detach().numpy()\n",
        "    preds = classifier.predict(out)\n",
        "    preds = torch.tensor(preds)\n",
        "    preds = preds.to(DEVICE)\n",
        "    for h in range(out.shape[0]):\n",
        "      if labels[h].item() == preds[h]:\n",
        "        running_corrects = running_corrects + 1\n",
        "    pred = torch.cat((pred,preds))\n",
        "    label = torch.cat((label,labels))\n",
        "pred_v = pred.cpu().detach().numpy()\n",
        "label_v = label.cpu().detach().numpy()\n",
        "del pred\n",
        "del label\n",
        "torch.cuda.empty_cache()\n",
        "cm = confusion_matrix(label_v, pred_v)\n",
        "cm = np.log1p(cm)\n",
        "df_cm = pd.DataFrame(cm, range(100), range(100))\n",
        "plt.figure(figsize=(5, 5))\n",
        "sn.heatmap(df_cm, annot=False, square=True, cbar=False, xticklabels=20, yticklabels=20)\n",
        "plt.xlabel('Predicted class')\n",
        "plt.ylabel('True class')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}