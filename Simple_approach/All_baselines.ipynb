{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICARL_ABLATION__COSINE (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWowydr8FQa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, random_split, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmUFqcrgFX0A",
        "colab_type": "text"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj4MHx74FThJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = 100  \n",
        "\n",
        "BATCH_SIZE = 128     \n",
        "                     \n",
        "K = 2000              \n",
        "LR = 2.0            \n",
        "LR_L2 = 0.25\n",
        "LR_CE = 0.1\n",
        "LR_LFC = 0.01\n",
        "LR_L1 = 0.1        \n",
        "MOMENTUM = 0.9       \n",
        "WEIGHT_DECAY = 1e-5  \n",
        "WEIGHT_DECAY_CE = 5e-4\n",
        "\n",
        "NUM_EPOCHS = 60      \n",
        "MILESTONES = [49, 63] \n",
        "MILESTONES_L2 = [30, 45]\n",
        "MILESTONES_CE = [50]\n",
        "MILESTONES_LFC = [50]\n",
        "GAMMA = 0.2       \n",
        "GAMMA_CE = 0.2\n",
        "GAMMA_LFC = 0.2\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "SEED = 1993"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVmu0DB2Ikvy",
        "colab_type": "text"
      },
      "source": [
        "**Transform**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFtCfK54Ikil",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(0.5),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                                                           std=[0.2673, 0.2564, 0.2761])])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                                                          std=[0.2673, 0.2564, 0.2761])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieNHuVP3Fev7",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hvmV4fYFV5a",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('./ICARL'):\n",
        "  !git clone https://github.com/lbonasera1/ICARL.git\n",
        "\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar import resnet32\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar_norelu import resnet32_norelu\n",
        "from ICARL.ResNet_CIFAR_100.resnet_cifar_cosine import resnet32_cosine\n",
        "\n",
        "train_dataset = CIFAR100(root='./ICARL/data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(root='./ICARL/data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_subsets = []\n",
        "test_subsets = []\n",
        "batch_classes = []\n",
        "class_indexes = [None] * NUM_CLASSES  \n",
        "random.seed(SEED)\n",
        "L = list(range(100))\n",
        "random.shuffle(L)\n",
        "\n",
        "for i in range(10):\n",
        "  # random extract 10 classes from 100\n",
        "  batch_classes.append([L.pop() for _ in range(10)])\n",
        "\n",
        "  # search and collect train and exemplars indices for i-batch\n",
        "  train_indices = []\n",
        "  for target in batch_classes[i]:\n",
        "    tmp = []\n",
        "    for idx, val in enumerate(train_dataset.targets):\n",
        "      if val == target:\n",
        "        train_indices.append(idx)\n",
        "        tmp.append(idx)\n",
        "    random.shuffle(tmp)\n",
        "    class_indexes[target] = tmp\n",
        "\n",
        "  random.shuffle(train_indices)\n",
        "  # create subset from indices\n",
        "  subset = Subset(train_dataset, train_indices)\n",
        "  train_subsets.append(subset)\n",
        "\n",
        "  # search and collect train indices for i-batch\n",
        "  test_indices = []\n",
        "  for target in batch_classes[i]:\n",
        "    for idx, val in enumerate(test_dataset.targets):\n",
        "      if target == val:\n",
        "         test_indices.append(idx)\n",
        "\n",
        "  random.shuffle(test_indices)\n",
        "  # create subset from indices\n",
        "  subset = Subset(test_dataset, test_indices)\n",
        "  test_subsets.append(subset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_JEvcyBRZuL",
        "colab_type": "text"
      },
      "source": [
        "**Function Joint Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flh7r2TrRiBC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "def jointTraining(lr, milestones, num_epochs, gamma, train_dataset, val_dataset):\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=32)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=32)\n",
        "  print(\"ok\")\n",
        "  net = resnet32()\n",
        "  # Define loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # Choose parameters to optimize\n",
        "  parameters_to_optimize = net.parameters()\n",
        "  # Define optimizer\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr, momentum=MOMENTUM, weight_decay=1e-4)\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma)\n",
        "  \n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "  accuracyList = []\n",
        "  train_lossList = []\n",
        "  val_lossList = []\n",
        "  tmp = []\n",
        "  acc_tmp = []\n",
        "\n",
        "  current_step = 0\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "    tmp.clear()\n",
        "    acc_tmp().clear()\n",
        "    # Iterate over the train dataset\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      net.train() # Sets module in training mode\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to Gy, from source\n",
        "      outputs = net(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "      # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "      current_step += 1\n",
        "\n",
        "    # Train loss mean\n",
        "    loss_mean = np.mean(tmp)\n",
        "    train_lossList.append(loss_mean)\n",
        "    # Train accuracy mean\n",
        "\n",
        "      \n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    # validation \n",
        "    tmp.clear()\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(val_dataloader):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Validation loss\n",
        "      loss = criterion(outputs, labels)\n",
        "      tmp.append(loss.item())\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate validation mean loss per epoch\n",
        "    val_lossList.append(np.mean(tmp))\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(val_dataset))\n",
        "    accuracyList.append(accuracy)\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.linspace(0,num_epochs,len(accuracyList)), accuracyList, c='blue', linestyle='-')\n",
        "  plt.title('Accuracy graph. SGD with LR = {}, NUM_EPOCHS = {}, STEP_SIZE = {}, Gamma = {}'.format(lr, num_epochs, milestones, gamma))\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  # plot loss graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.linspace(0,num_epochs,len(train_lossList)), train_lossList, c='green', linestyle='-')\n",
        "  ax.plot(np.linspace(0,num_epochs,len(val_lossList)), val_lossList, c='red', linestyle='-')\n",
        "  plt.title('Loss graph. SGD with LR = {}, NUM_EPOCHS = {}, STEP_SIZE = {}, Gamma = {}'.format(lr, num_epochs, milestones, gamma))\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  score = np.max(accuracyList)\n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2969SygPGE0",
        "colab_type": "text"
      },
      "source": [
        "**Create loss target**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOYwa2ezPF1T",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def lossTarget(labels, output_old=None, labels_old=None):\n",
        "  tensor = torch.zeros((labels.size(0), NUM_CLASSES), device=\"cuda:0\")\n",
        "  for i in range(labels.size(0)):\n",
        "    # one-hot for new classes\n",
        "    tensor.data[i][labels[i]] = 1\n",
        "    # distillation from previous classes\n",
        "    if output_old is not None or labels_old is not None:\n",
        "      for j in labels_old:\n",
        "        tensor.data[i][j] = output_old.data[i][j]          \n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trcilk1-d2JV",
        "colab_type": "text"
      },
      "source": [
        "**Function Fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "lr_uk1P6CS_Q",
        "colab": {}
      },
      "source": [
        "def fineTuning(train_subsets, test_subsets):\n",
        "  net = resnet32()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  subsets = []\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "\n",
        "    # concatenate test classes\n",
        "    subsets.append(test_subsets[i])\n",
        "    \n",
        "    train_subset = train_subsets[i]\n",
        "    test_subset = ConcatDataset(subsets)\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    train_lossList = []\n",
        "    tmp = []\n",
        "\n",
        "    if i == 0:\n",
        "      parameters_to_optimize = net.parameters()\n",
        "      optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "      scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "    else:\n",
        "      parameters_to_optimize = net.fc.parameters()\n",
        "      optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "      scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i))\n",
        "      tmp.clear()\n",
        "      # Iterate over the train dataset\n",
        "      for images, labels in train_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train() \n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        outputs = net(images)\n",
        "        tensor = lossTarget(labels)\n",
        "        loss = criterion(outputs, tensor)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # Train loss mean\n",
        "      loss_mean = np.mean(tmp)\n",
        "      train_lossList.append(loss_mean)\n",
        "\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "        \n",
        "    net.train(False)\n",
        "\n",
        "    # test\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_subset))\n",
        "    batches_accuracy.append(accuracy)\n",
        "    print(\"Accuracy on batch {}: {}\".format(i+1, accuracy))\n",
        "\n",
        "    # plot loss graph\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_lossList, c='green', linestyle='-', label='Train loss')\n",
        "    plt.title('Loss graph. SGD with LR = {}, NUM_EPOCHS = {}, MILESTONES = {}, Gamma = {}'.format(LR, NUM_EPOCHS, MILESTONES, GAMMA))\n",
        "    plt.tight_layout()\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='*')\n",
        "  plt.grid()\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRPAhb0KNeiH",
        "colab_type": "text"
      },
      "source": [
        "**Loading previous model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB2xtBZSNb6y",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "  model = torch.load(filepath)\n",
        "  for parameter in model.parameters():\n",
        "      parameter.requires_grad = False\n",
        "  model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qW__z5Bda1a",
        "colab_type": "text"
      },
      "source": [
        "**Function LwF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "zSA_aPAnduF-",
        "colab": {}
      },
      "source": [
        "def LwF(train_subsets, test_subsets, batch_classes):\n",
        "  net = resnet32()\n",
        "  # Define loss function\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  subsets = []\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "\n",
        "    # concatenate test classes\n",
        "    subsets.append(test_subsets[i])\n",
        "    \n",
        "    train_subset = train_subsets[i]\n",
        "    test_subset = ConcatDataset(subsets)\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    train_lossList = []\n",
        "    tmp = []\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
        "      tmp.clear()\n",
        "      # Iterate over the train dataset\n",
        "      for images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "\n",
        "        tensor = lossTarget(labels=labels)\n",
        "        if i > 0:\n",
        "          # loading pre-update parameters for distillation\n",
        "          prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "          outputs_old = torch.sigmoid(prev_net(images))\n",
        "          # generate target\n",
        "          tensor = lossTarget(labels=labels, output_old=outputs_old, labels_old=labels_old)\n",
        "\n",
        "        loss = criterion(outputs, tensor)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # Train loss mean\n",
        "      loss_mean = np.mean(tmp)\n",
        "      train_lossList.append(loss_mean)\n",
        "\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "        \n",
        "    net.train(False)\n",
        "    # test\n",
        "    running_corrects = 0 \n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_subset))\n",
        "    batches_accuracy.append(accuracy)\n",
        "    print(\"Accuracy on batch {}: {}\".format(i+1, accuracy))\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "    for j in batch_classes[i]:\n",
        "      labels_old.append(j)\n",
        "\n",
        "    # plot loss graph\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_lossList, c='green', linestyle='-', label='Train loss')\n",
        "    plt.title('Loss graph. SGD with LR = {}, NUM_EPOCHS = {}, MILESTONES = {}, Gamma = {}'.format(LR, NUM_EPOCHS, MILESTONES, GAMMA))\n",
        "    plt.tight_layout()\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='*')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJAOZcbjDy1K",
        "colab_type": "text"
      },
      "source": [
        "**Hybrid 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "CF6NTL0pDxYE",
        "colab": {}
      },
      "source": [
        "def hybrid1(train_subsets, test_subsets, batch_classes, class_indexes):\n",
        "  net = resnet32()\n",
        "  # Define loss function\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "  indices = copy.deepcopy(class_indexes)\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "\n",
        "    # get old labels\n",
        "    if i > 0:\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "         train_subList.append(Subset(train_dataset, exemplars[k]))\n",
        "      train_subList.append(train_subset)\n",
        "      train_subset = ConcatDataset(train_subList)\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    train_lossList = []\n",
        "    tmp = []\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i))\n",
        "      tmp.clear()\n",
        "      # Iterate over the train dataset\n",
        "      for images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "\n",
        "        if i > 0:\n",
        "          # loading pre-update parameters for distillation\n",
        "          with torch.no_grad():\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            outputs_old = torch.sigmoid(prev_net(images))\n",
        "            # generate target\n",
        "            tensor = lossTarget(labels=labels, output_old=outputs_old, labels_old=labels_old)\n",
        "        else:\n",
        "          tensor = lossTarget(labels=labels)\n",
        "\n",
        "        loss = criterion(outputs, tensor)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # Train loss mean\n",
        "      loss_mean = np.mean(tmp)\n",
        "      train_lossList.append(loss_mean)\n",
        "        \n",
        "    net.train(False)\n",
        "\n",
        "    # test\n",
        "    tmp.clear()\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_subset))\n",
        "    batch_accuracy.append(accuracy)\n",
        "    print(\"Accuracy on batch {}: {}\".format(i+1, accuracy))\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "\n",
        "    # construct random exemplars with current classes\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      for k in labels_old:\n",
        "        exemplars[k] = exemplars[k][:m]\n",
        "    for k in batch_classes[i]:\n",
        "      exemplars[k] = [indices[k].pop(random.randrange(len(indices[k]))) for _ in range(m)]\n",
        "\n",
        "    # plot loss graph\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_lossList, c='green', linestyle='-', label='Train loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.title('Loss graph. SGD with LR = {}, NUM_EPOCHS = {}, MILESTONES = {}, Gamma = {}'.format(LR, NUM_EPOCHS, MILESTONES, GAMMA))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='*')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_6ceSWeMRvB",
        "colab_type": "text"
      },
      "source": [
        "**Construct Exemplars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaKeejAPMQwX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def constrExemplars(exemplars, classes, class_indexes, model, m):\n",
        "  pdist = nn.PairwiseDistance(p=2)\n",
        "  model.train(False)\n",
        "  model.set_flag(False)\n",
        "  class_means = torch.empty((0, 64)).cuda()\n",
        "  with torch.no_grad():\n",
        "    # compute mean for each class\n",
        "    for c in classes:\n",
        "      indexes = copy.deepcopy(class_indexes[c])\n",
        "      features = torch.empty((0, 64)).cuda()\n",
        "      # image set of class c\n",
        "      subset = Subset(train_dataset, indexes)\n",
        "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "      for image, label in dataLoader:\n",
        "        image = image.to(DEVICE)\n",
        "        # extract features\n",
        "        output = model(image)\n",
        "        # L2 normalization of feature vector\n",
        "        output = nn.functional.normalize(output, p=2, dim=1)\n",
        "        features = torch.cat((features, output))\n",
        "      \n",
        "      class_mean = torch.mean(features, 0)\n",
        "      class_mean = nn.functional.normalize(class_mean, p=2, dim=0)\n",
        "      class_mean = class_mean.view(-1, 64)\n",
        "      class_means = torch.cat((class_means, class_mean))\n",
        "      current_features = torch.empty((0, 64)).cuda()\n",
        "      exemplars_indexes = []\n",
        "      for k in range(m):\n",
        "        current_sum = torch.sum(current_features, 0)\n",
        "        current_sum = torch.add(features, current_sum.repeat(features.size(0), 1))\n",
        "        current_mean = current_sum * (1.0/(k+1))\n",
        "        current_mean = nn.functional.normalize(current_mean, p=2, dim=1)\n",
        "        distances = pdist(current_mean, class_mean)\n",
        "        index = torch.argmin(distances).item()   \n",
        "        phi = features[index].view(-1, 64)\n",
        "        # collecting chosen features\n",
        "        current_features = torch.cat((current_features, phi))\n",
        "        # removing chosen features\n",
        "        features = torch.cat((features[:index], features[index+1:]))\n",
        "        exemplars_indexes.append(indexes.pop(index))  \n",
        "      exemplars[c] = exemplars_indexes\n",
        "  model.set_flag(True)\n",
        "  model.train()\n",
        "  return class_means\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTrzbBmSvvMd",
        "colab_type": "text"
      },
      "source": [
        "**Reduce Exemplars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1YO1b99vu9R",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def reduceExemplars(exemplars, classes, m):\n",
        "  for c in classes:\n",
        "    exemplars[c] = exemplars[c][:m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B6yi_aJQAPk",
        "colab_type": "text"
      },
      "source": [
        "**Hybrid 1 with exemplar chosen by mean**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "ANqI7xf8P-8O",
        "colab": {}
      },
      "source": [
        "def hybrid1Mean(train_subsets, test_subsets, batch_classes, class_indexes):\n",
        "  net = resnet32()\n",
        "  # Define loss function\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "    # get old labels\n",
        "    if i > 0:\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "        train_subList.append(Subset(train_dataset, exemplars[k]))\n",
        "      train_subList.append(train_subset)\n",
        "      train_subset = ConcatDataset(train_subList)\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "    train_lossList = []\n",
        "    tmp = []\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i))\n",
        "      tmp.clear()\n",
        "      # Iterate over the train dataset\n",
        "      for images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "\n",
        "        if i > 0:\n",
        "          # loading pre-update parameters for distillation\n",
        "          with torch.no_grad():\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            outputs_old = torch.sigmoid(prev_net(images))\n",
        "            # generate target\n",
        "            tensor = lossTarget(labels=labels, output_old=outputs_old, labels_old=labels_old)\n",
        "        else:\n",
        "          tensor = lossTarget(labels=labels)\n",
        "\n",
        "        loss = criterion(outputs, tensor)\n",
        "        tmp.append(loss.item())\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # Train loss mean\n",
        "      loss_mean = np.mean(tmp)\n",
        "      train_lossList.append(loss_mean)\n",
        "        \n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    # test\n",
        "    tmp.clear()\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_subset))\n",
        "    batches_accuracy.append(accuracy)\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "\n",
        "    # construct exemplars with current classes\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m)\n",
        "    constrExemplars(exemplars=exemplars, classes=batch_classes[i], class_indexes=class_indexes, model=net, m=m)\n",
        "    \n",
        "    # plot loss graph\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.plot(np.arange(0, NUM_EPOCHS), train_lossList, c='green', linestyle='-', label='Train loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.title('Loss graph. SGD with LR = {}, NUM_EPOCHS = {}, MILESTONES = {}, Gamma = {}'.format(LR, NUM_EPOCHS, MILESTONES, GAMMA))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.arange(0, 100, 10), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Yo8MAVRUIU",
        "colab_type": "text"
      },
      "source": [
        "**NCM Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_vtXFU2RQXv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def classifierNCM(input_image, exemplars, labels_old, labels_new, class_means, model):\n",
        "  pdist = nn.PairwiseDistance(p=2)\n",
        "  model.train(False)\n",
        "  model.set_flag(False)\n",
        "  with torch.no_grad():\n",
        "    tensor = torch.zeros((input_image.size(0), NUM_CLASSES), device=\"cuda:0\")\n",
        "    exemplars_mean = torch.empty((0, 64)).cuda()\n",
        "    for c in labels_old:\n",
        "      subset = Subset(train_dataset, exemplars[c])\n",
        "      dataLoader = DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "      mean = torch.empty((0, 64)).cuda()\n",
        "      for image, label in dataLoader:\n",
        "        image = image.to(DEVICE)\n",
        "        output = model(image)\n",
        "        # L2 normalization of feature vector\n",
        "        output = nn.functional.normalize(output, p=2, dim=1)\n",
        "        mean = torch.cat((mean, output))\n",
        "      mean = torch.mean(mean, 0)\n",
        "      mean = nn.functional.normalize(mean, p=2, dim=0)\n",
        "      mean = mean.view(-1, 64)\n",
        "      exemplars_mean = torch.cat((exemplars_mean, mean))\n",
        "\n",
        "    exemplars_mean = torch.cat((exemplars_mean, class_means))\n",
        "    classes = labels_old + labels_new\n",
        "\n",
        "    output = model(input_image)\n",
        "    output = nn.functional.normalize(output, p=2, dim=1)\n",
        "    for n in range((output.size(0))):\n",
        "      image = output[n]\n",
        "      distances = pdist(image, exemplars_mean)\n",
        "      index = torch.argmin(distances).item()\n",
        "      index = classes[index]\n",
        "      tensor[n][index] = 1\n",
        "\n",
        "  model.set_flag(True)\n",
        "  model.train()\n",
        "  return tensor  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HUlq2c1_XYl7"
      },
      "source": [
        "**ICaRL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "UtdyQX4hXfBK",
        "colab": {}
      },
      "source": [
        "def icarl(train_subsets, test_subsets, batch_classes, class_indexes):\n",
        "  net = resnet32()\n",
        "  # Define loss function\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  net = net.to(DEVICE)\n",
        "  cudnn.benchmark\n",
        "  batches_accuracy = []\n",
        "  labels_old = []\n",
        "  test_subList = []\n",
        "  exemplars = [None] * NUM_CLASSES\n",
        "\n",
        "  # iterate over class batches\n",
        "  for i in range(10):\n",
        "    # concatenate test classes\n",
        "    test_subList.append(test_subsets[i])\n",
        "    test_subset = ConcatDataset(test_subList)\n",
        "    # adding exemplars to train subset\n",
        "    train_subset = train_subsets[i]\n",
        "    if i > 0:\n",
        "      # get old labels\n",
        "      for j in batch_classes[i-1]:\n",
        "        labels_old.append(j)\n",
        "      train_subList = []\n",
        "      for k in labels_old:\n",
        "        train_subList = train_subList + exemplars[k]\n",
        "      random.shuffle(train_subList)  \n",
        "      subset = Subset(train_dataset, train_subList)\n",
        "      train_subset = ConcatDataset([train_subset, subset])\n",
        "    # initializate dataloader and variables\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print('Starting epoch {}/{}, LR = {}, Batch {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr(), i+1))\n",
        "      # Iterate over the train dataset\n",
        "      for images, labels in train_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        net.train() # Sets module in training mode\n",
        "        optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "        outputs = net(images)\n",
        "\n",
        "        if i > 0:\n",
        "          # loading pre-update parameters for distillation\n",
        "          with torch.no_grad():\n",
        "            prev_net = load_checkpoint('./ICARL/prev_net.pt')\n",
        "            outputs_old = torch.sigmoid(prev_net(images))\n",
        "            # generate target\n",
        "            tensor = lossTarget(labels=labels, output_old=outputs_old, labels_old=labels_old)\n",
        "        else:\n",
        "          tensor = lossTarget(labels=labels)\n",
        "\n",
        "        loss = criterion(outputs, tensor)\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "      # Step the scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "    # reducing previous exemplars\n",
        "    m = K // (10*(i+1))\n",
        "    if i > 0:\n",
        "      reduceExemplars(exemplars=exemplars, classes=labels_old, m=m) \n",
        "\n",
        "    # construct exemplars with current classes\n",
        "    class_means = constrExemplars(exemplars, batch_classes[i], class_indexes, net, m)\n",
        "\n",
        "    net.train(False)\n",
        "\n",
        "    # test\n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      with torch.no_grad():\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = classifierNCM(images, exemplars, labels_old, batch_classes[i], class_means, net)\n",
        "        # outputs = net(images)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    score = running_corrects / float(len(test_subset))\n",
        "    # saving i-batch model parameters (distillation)\n",
        "    torch.save(net, './ICARL/prev_net.pt')\n",
        "    # accuracy of last epoch model\n",
        "    print(\"Test accuracy of batch {} equal to: {}\".format(i+1, score))\n",
        "    batches_accuracy.append(score)\n",
        "  \n",
        "  # plot accuracy graph\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.plot(np.linspace(0,NUM_CLASSES,len(batches_accuracy)), batches_accuracy, c='blue', linestyle='-', marker='.')\n",
        "  plt.title('Accuracy graph vs Number of classes')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return batches_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZq0DUNeOs7G",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJnfYZh0YmzC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Fine-Tuning\n",
        "# scores = fineTuning(train_subsets, test_subsets)\n",
        "\n",
        "# LwF\n",
        "# scores = LwF(train_subsets, test_subsets, batch_classes)\n",
        "\n",
        "# Hybrid1\n",
        "# scores = hybrid1(train_subsets, test_subsets, batch_classes, class_indexes)\n",
        "\n",
        "# Hybrid1 mean exemplars\n",
        "# scores = hybrid1Mean(train_subsets, test_subsets, batch_classes, class_indexes)\n",
        "\n",
        "# Icarl\n",
        "# scores = icarl(train_subsets, test_subsets, batch_classes, class_indexes)\n",
        "\n",
        "# print(scores)\n",
        "# print(\"Average multi-class accuracy: {}\".format(np.mean(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}